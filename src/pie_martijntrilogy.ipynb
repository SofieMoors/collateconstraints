{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b702bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlp-pie\n",
      "  Using cached nlp_pie-0.3.8-py2.py3-none-any.whl (84 kB)\n",
      "Collecting pyyaml==5.1b3\n",
      "  Using cached PyYAML-5.1b3.tar.gz (273 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nlp-pie\n",
      "  Using cached nlp_pie-0.3.7-py2.py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: typing<4.0 in /Users/sofiemoors/miniconda3/lib/python3.10/site-packages (from nlp-pie) (3.7.4.3)\n",
      "Collecting scikit-learn<0.23.0,>=0.19.1\n",
      "  Using cached scikit-learn-0.22.2.post1.tar.gz (6.9 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nlp-pie\n",
      "  Using cached nlp_pie-0.3.6-py2.py3-none-any.whl (84 kB)\n",
      "  Using cached nlp_pie-0.3.5-py2.py3-none-any.whl (84 kB)\n",
      "  Using cached nlp_pie-0.3.4-py2.py3-none-any.whl (84 kB)\n",
      "  Using cached nlp_pie-0.3.3-py2.py3-none-any.whl (83 kB)\n",
      "  Using cached nlp_pie-0.3.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting tqdm==4.23.3\n",
      "  Using cached tqdm-4.23.3-py2.py3-none-any.whl (42 kB)\n",
      "Collecting gensim==3.4.0\n",
      "  Using cached gensim-3.4.0.tar.gz (22.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lxml==4.2.1\n",
      "  Using cached lxml-4.2.1.tar.gz (4.3 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nlp-pie\n",
      "  Using cached nlp_pie-0.3.1-py2.py3-none-any.whl (82 kB)\n",
      "  Using cached nlp_pie-0.3.0-py2.py3-none-any.whl (82 kB)\n",
      "  Using cached nlp_pie-0.2.8-py2.py3-none-any.whl (79 kB)\n",
      "  Using cached nlp_pie-0.2.7-py2.py3-none-any.whl (78 kB)\n",
      "  Using cached nlp_pie-0.2.6-py2.py3-none-any.whl (78 kB)\n",
      "  Using cached nlp_pie-0.2.5-py2.py3-none-any.whl (78 kB)\n",
      "Collecting numpy<1.18.0,>=1.14.3\n",
      "  Using cached numpy-1.17.5.zip (6.4 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: JSON-minify==0.3.0 in /Users/sofiemoors/miniconda3/lib/python3.10/site-packages (from nlp-pie) (0.3.0)\n",
      "Requirement already satisfied: click<8.0,>=7.0 in /Users/sofiemoors/miniconda3/lib/python3.10/site-packages (from nlp-pie) (7.1.2)\n",
      "Collecting termcolor==1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting terminaltables==3.1.0\n",
      "  Using cached terminaltables-3.1.0.tar.gz (12 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nlp-pie\n",
      "  Using cached nlp_pie-0.2.2-py2.py3-none-any.whl (79 kB)\n",
      "\u001b[31mERROR: Cannot install nlp-pie==0.2.2, nlp-pie==0.2.5, nlp-pie==0.2.6, nlp-pie==0.2.7, nlp-pie==0.2.8, nlp-pie==0.3.0, nlp-pie==0.3.1, nlp-pie==0.3.2, nlp-pie==0.3.3, nlp-pie==0.3.4, nlp-pie==0.3.5, nlp-pie==0.3.6, nlp-pie==0.3.7 and nlp-pie==0.3.8 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    nlp-pie 0.3.8 depends on torch<=1.7.1 and >=1.3.1\n",
      "    nlp-pie 0.3.7 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.3.6 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.3.5 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.3.4 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.3.3 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.3.2 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.3.1 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.3.0 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.2.8 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.2.7 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.2.6 depends on torch<1.4.0 and >=1.3.1\n",
      "    nlp-pie 0.2.5 depends on torch<1.2.0 and >=1.0.1\n",
      "    nlp-pie 0.2.2 depends on torch<=1.2.0 and >=1.0.1\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nlp-pie\n",
    "#!pip install pie-extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c4d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pie_extended.cli.utils import get_tagger, get_model, download\n",
    "from pie_extended.models. dum.imports import get_iterator_and_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3adbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n",
      "[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "do_download = True\n",
    "if do_download:\n",
    "    for dl in download(\"dum\"):\n",
    "        x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7da1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator, processor = get_iterator_and_processor()\n",
    "tagger = get_tagger('dum', batch_size=2048, device=\"cpu\", model_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae5ffeae-ffc7-4394-bf8b-6efc25cf2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob #alle bestandspaden terugvinden die overeenkomen met specifiek patroon\n",
    "import os #aanmaken van mappen, verwijderen van mappen,...\n",
    "from itertools import combinations #reeks verzameling cijfers/letters, combinaties in lexicografische volgorde\n",
    "import grapheme #voor stringmanipulatie & rekenfucties -> stel je hebt x = 'n̄', dan len(x) = 2 (want 2 bytes), grapheme.length(x) is dan wel 1\n",
    "from collatex import * #alle functies van collatex importeren, * geeft weer dat het om alle functies gaat\n",
    "from tqdm import tqdm #progress meters toevoegen\n",
    "import numpy as np #numpy dient om met lijsten/arrays te werken \n",
    "import pandas as pd #gebruik maken van de functies van Panda, zoals pd.DataFrame\n",
    "import seaborn as sb #statistical data visualization\n",
    "import matplotlib.pyplot as plt #creating static/animated/interactive visualizations\n",
    "\n",
    "from lxml import etree\n",
    "from re import sub #re — Regular expression operations #\n",
    "import xml.etree.ElementTree as ET\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f5f368c-dc0e-4d1d-befc-1a27eb1f76b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'Ant',\n",
       " 'B',\n",
       " 'Br',\n",
       " 'C',\n",
       " 'D',\n",
       " 'D2',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'Ge',\n",
       " 'K',\n",
       " 'L',\n",
       " 'O',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'Z']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigles = [os.path.basename(fn).replace('xml_', '').replace('.xml', '') for fn in glob('../collateconstraints/data/xml_martijn_lemmatizer/*.xml')] \n",
    "sigles = sorted(sigles)\n",
    "sigles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625297bb-220d-4806-ab87-f067ddf5c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gap_lines(tree):\n",
    "    gap_lines = []\n",
    "    for text in tree.iterfind('.//'+\"{\"+ NSMAP[\"MVN\"]+ \"}\"+'text'):\n",
    "        if 'n' in text.attrib:\n",
    "            for line in text.iterfind('.//'+\"{\"+ NSMAP[\"MVN\"]+ \"}\"+'l'):\n",
    "                if(line.find('.//'+\"{\"+ NSMAP[\"MVN\"]+ \"}\"+'gap')) is not None:\n",
    "                    gap_lines.append(line.attrib['n'])\n",
    "                 #   text.attrib['n'] + '-' + \n",
    "    return gap_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d665e4-a5f9-4fe2-a1f5-8d130c79975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSMAP = {'MVN': 'http://www.tei-c.org/ns/1.0'} \n",
    "removes = ('teiHeader', 'fw', 'supplied', 'abbr') \n",
    "removes_expan_false = ('teiHeader', 'fw', 'supplied', 'ex', 'expan')\n",
    "chars = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l','m', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'}       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8a6e87-918e-44b3-8d4e-4f67362a120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1766\n"
     ]
    }
   ],
   "source": [
    "def extract_lines(xml_file, expan = True, \n",
    "                  punct = True, lower = True,\n",
    "                  sep_abbr = True): \n",
    "    \n",
    "    lines = {}\n",
    "    damaged_lines = {}\n",
    "    tree = etree.parse(xml_file) \n",
    "     \n",
    "    if expan:\n",
    "        etree.strip_elements(tree, (\"{\"+ NSMAP[\"MVN\"]+ \"}\" + s for s in removes), with_tail=False) \n",
    "    else: \n",
    "        etree.strip_elements(tree, (\"{\"+ NSMAP[\"MVN\"]+ \"}\" + s for s in removes_expan_false), with_tail=False)\n",
    "  \n",
    "    context = etree.iterwalk(tree, events=(\"start\", \"end\")) \n",
    "    text = u\"\" \n",
    "    k = ''     \n",
    "    \n",
    "    for action, node in context:   \n",
    "        tag_only = node.tag.replace(\"{http://www.tei-c.org/ns/1.0}\",\"\") \n",
    "\n",
    "        if 'n' in node.attrib and tag_only == 'text': \n",
    "            title = node.attrib['n'] \n",
    "    \n",
    "        if 'n' in node.attrib and tag_only == \"l\":   \n",
    "            k = node.attrib['n']\n",
    "            #print (k)\n",
    "           # title + '-' + \n",
    "            \n",
    "        if action == 'start' and tag_only == 'text': \n",
    "            continue\n",
    "            \n",
    "        elif action == 'start' and tag_only == 'lg':\n",
    "            continue \n",
    "  \n",
    "        elif action == 'start' and tag_only == 'lb':\n",
    "            continue\n",
    "\n",
    "        elif tag_only in (\"group\",\"text\",\"MVN\",\"body\",\"cb\",\"p\"):\n",
    "            continue\n",
    "            \n",
    "         \n",
    "        elif action == 'start':\n",
    "                    \n",
    "            if tag_only == 'g':\n",
    "                if sep_abbr: \n",
    "                    if node.attrib['ref'] == '#bar': # ā, ē, ī, ō, ū, n̄ etc.\n",
    "                        text += u'\\u005f' #low line _\n",
    "\n",
    "                    elif node.attrib['ref'] == '#apomod': # ʼ\n",
    "                        text += u'\\u02bc'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#usmod': # ꝰ\n",
    "                        text += u'\\ua770' \n",
    "\n",
    "                    elif node.attrib['ref'] == '#condes': # ꝯ\n",
    "                        text += u'\\ua76f'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#para': # ¶\n",
    "                        text += u'\\xb6'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#etfin': # ꝫ\n",
    "                        text += u'\\ua76b'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#pbardes': # ꝑ\n",
    "                        text += '\\ua751'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#pbardes': # ꝕ\n",
    "                        text += u'\\ua755'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#pflour': # ꝓ\n",
    "                        text += u'\\ua753'\n",
    "\n",
    "                    else:\n",
    "                        text += str(node.attrib['ref']) # get the actual ref if there still are any left\n",
    "                    \n",
    "                else:\n",
    "                    if node.attrib['ref'] == '#bar': # ā, ē, ī, ō, ū, n̄ etc.\n",
    "                        text += u'\\u0304'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#apomod': # ʼ\n",
    "                        text += u'\\u02bc'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#usmod': # ꝰ\n",
    "                        text += u'\\ua770'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#condes': # ꝯ\n",
    "                        text += u'\\ua76f'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#para': # ¶\n",
    "                        text += u'\\xb6'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#etfin': # ꝫ\n",
    "                        text += u'\\ua76b'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#pbardes': # ꝑ\n",
    "                        text += u'\\ua751'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#pbardes': # ꝕ\n",
    "                        text += u'\\ua755'\n",
    "\n",
    "                    elif node.attrib['ref'] == '#pflour': # ꝓ\n",
    "                        text += u'\\ua753'\n",
    "\n",
    "                    else:\n",
    "                        node.attrib['ref']\n",
    "                        text += str(node.attrib['ref']) \n",
    "            superscript_dict = {'a':'ᵃ', 'b':'ᵇ', 'c':'ᶜ', 'd':'ᵈ', 'e':'ᵉ', 'f':'ᶠ',\n",
    "                               'g':'ᵍ', 'h':'ʰ', 'i':'ᶦ', 'j':'ʲ', 'k':'ᵏ', 'l':'ˡ', \n",
    "                                'm':'ᵐ', 'n':'ⁿ', 'o':'ᵒ', 'p':'ᵖ', 'r':'ʳ', 's':'ˢ', \n",
    "                                't':'ᵗ', 'u':'ᵘ', 'v':'ᵛ', 'w':'ʷ', 'x':'ˣ', 'y': 'ʸ', 'z': 'ᶻ'}\n",
    "\n",
    "            if tag_only == 'hi' and 'rend' in node.attrib and node.attrib['rend'] == 'superscript': #rend(ition) supplies information about the appearance of an element\n",
    "                if node.text in superscript_dict:\n",
    "                    text += str(superscript_dict[node.text]).strip()\n",
    "\n",
    "            elif tag_only == 'pc':\n",
    "                text += str(node.text).strip()\n",
    "\n",
    "            elif tag_only == 'num':\n",
    "                if node.text:\n",
    "                    text += str('.'+node.text+'.').strip()\n",
    "\n",
    "            elif (node.text):\n",
    "                text += node.text        \n",
    "        elif action == 'end':\n",
    "            \n",
    "            if (node.tail and node.tail not in \"\\t\"): \n",
    "                text += str(node.tail)\n",
    "                \n",
    "        if tag_only == 'lb':\n",
    "            if k:\n",
    "                text = sub(r'\\n', '', text) \n",
    "                if not punct:\n",
    "                    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "                if lower: \n",
    "                    text = text.lower()\n",
    "                   \n",
    "                    #text = text[::-1]\n",
    "                  \n",
    "                lines[k] = text #the value of the keys k is the text \n",
    "                text = ''  \n",
    "                   \n",
    "   \n",
    "    if text:\n",
    "        lines[k] = (text)\n",
    "   \n",
    "    num_orig_lines = len(lines)\n",
    "    print(num_orig_lines)\n",
    "    # remove lines with gaps:\n",
    "    gap_lines = get_gap_lines(tree)\n",
    "    #print(gap_lines)\n",
    "    #lines = {k:v for k, v in lines.items()}\n",
    "    for k, v in lines.items():\n",
    "        if k in gap_lines:\n",
    "            damaged_lines[k] = ('True')\n",
    "        else:\n",
    "            damaged_lines[k] = ('False')\n",
    "    #lines = {k:v for k, v in lines.items() if v.strip()} #if a line with a gap is removed, remove empty key, strip() removes spaces #The items() method returns a key-value pair\n",
    "    \n",
    "    return lines, damaged_lines\n",
    "   \n",
    "\n",
    "d = extract_lines(f'../collateconstraints/data/xml_martijn_lemmatizer/xml_{sigles[0]}.xml', expan = True, punct = False, lower = True)\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ec684f-5678-4e16-b0f9-a0035a20de74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1766\n",
      "104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████▎                                 | 4/17 [00:00<00:00, 34.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816\n",
      "606\n",
      "1472\n",
      "1811\n",
      "247\n",
      "276\n",
      "1821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████▎                 | 10/17 [00:00<00:00, 35.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1768\n",
      "152\n",
      "67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 17/17 [00:00<00:00, 44.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703\n",
      "1821\n",
      "507\n",
      "348\n",
      "534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#for sigle in sigles: \n",
    "  #  d = extract_lines(f'../collateconstraints/data/xml_martijn/xml_{sigle}.xml', expan = True, punct = False, lower = True)\n",
    "    #print(d)\n",
    "\n",
    "mss = {} #dictionary maken van manuscripten\n",
    "\n",
    "for sigle in tqdm(sigles): #for... in = loop, toont progress meter \n",
    "    mss[sigle] = extract_lines(f'../collateconstraints/data/xml_martijn_lemmatizer/xml_{sigle}.xml',\n",
    "                               expan = True, punct = False, lower = True,\n",
    "                               sep_abbr = False)\n",
    "\n",
    "#print(mss['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981994c7-d49e-4537-853c-ed87a67d1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "keys = []  # Create an empty list to store the keys\n",
    "\n",
    "for sigle in sigles: \n",
    "    for key, line in mss[sigle][0].items():  # Use the items() method to iterate over both keys and values\n",
    "        clean_line = []\n",
    "        for token in line.split():\n",
    "            token = ''.join([c for c in token.lower() if c.isalpha()])\n",
    "            clean_line.append(token)\n",
    "        lines.append(' '.join(clean_line))\n",
    "        keys.append(key)  # Append the key to the list\n",
    "# Print keys and lines together\n",
    "    \n",
    "#for key, line in zip(keys, lines):\n",
    "  #  print(key, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ee39a6-7276-4d02-b625-8a6bdb95abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "for sigle in sigles:\n",
    "    filename = f\"data/tsv_files/{sigle}_output.tsv\"\n",
    "    lines = []\n",
    "    keys = []  # Create an empty list to store the keys\n",
    "\n",
    "    for key, line in mss[sigle][0].items():  # Use the items() method to iterate over both keys and values\n",
    "        clean_line = []\n",
    "        for token in line.split():\n",
    "            token = ''.join([c for c in token.lower() if c.isalpha()])\n",
    "            clean_line.append(token)\n",
    "        lines.append(' '.join(clean_line))\n",
    "        keys.append(key)  # Append the key to the list\n",
    "\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t', quoting=csv.QUOTE_NONE, escapechar='')\n",
    "\n",
    "            # Write the header row\n",
    "        header_row = ['line_id', 'form', 'lemma', 'pos', 'corr_lemma', 'corr_pos', 'damage']  # Include 'key' in the header\n",
    "        writer.writerow(header_row)\n",
    "\n",
    "        for key, line in zip(keys, lines):  # Iterate over both keys and lines together\n",
    "            damage = mss[sigle][1][key]\n",
    "            for w in tagger.tag_str(line.lower(), iterator=iterator, processor=processor):\n",
    "                # Write each dictionary as a row in the TSV file, including the key\n",
    "                writer.writerow([key, w['form'], w['lemma'], w['pos'], w['lemma'], w['pos'], damage])\n",
    "\n",
    "            # Add an empty line between each dictionary\n",
    "            writer.writerow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662ef5c-a32f-43f7-b76b-3c7af9c9a9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
